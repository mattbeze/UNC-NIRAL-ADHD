{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras import losses\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "from pandas import DataFrame\n",
    "import xlsxwriter\n",
    "\n",
    "ct_sheet = pd.ExcelFile(\"Training Data.xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mattbeze\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\base.py:334: UserWarning: Trying to unpickle estimator MinMaxScaler from version 0.24.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Users\\mattbeze\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\base.py:334: UserWarning: Trying to unpickle estimator MinMaxScaler from version 0.24.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(290, 148)\n",
      "(290, 148)\n",
      "Tensor(\"input_1:0\", shape=(None, 148), dtype=float32)\n",
      "Epoch 1/150\n",
      "290/290 [==============================] - 4s 14ms/step - loss: 0.2270\n",
      "Epoch 2/150\n",
      "290/290 [==============================] - 0s 1ms/step - loss: 0.1386\n",
      "Epoch 3/150\n",
      "290/290 [==============================] - 0s 1ms/step - loss: 0.1264\n",
      "Epoch 4/150\n",
      "290/290 [==============================] - 0s 1ms/step - loss: 0.1193\n",
      "Epoch 5/150\n",
      "290/290 [==============================] - 0s 1ms/step - loss: 0.1168\n",
      "Epoch 6/150\n",
      "290/290 [==============================] - 0s 941us/step - loss: 0.1156\n",
      "Epoch 7/150\n",
      "290/290 [==============================] - 0s 774us/step - loss: 0.1136\n",
      "Epoch 8/150\n",
      "290/290 [==============================] - 0s 752us/step - loss: 0.1124\n",
      "Epoch 9/150\n",
      "290/290 [==============================] - 0s 838us/step - loss: 0.1101\n",
      "Epoch 10/150\n",
      "290/290 [==============================] - 0s 716us/step - loss: 0.1096\n",
      "Epoch 11/150\n",
      "290/290 [==============================] - 0s 790us/step - loss: 0.1095\n",
      "Epoch 12/150\n",
      "290/290 [==============================] - 0s 705us/step - loss: 0.1085\n",
      "Epoch 13/150\n",
      "290/290 [==============================] - 0s 685us/step - loss: 0.1073\n",
      "Epoch 14/150\n",
      "290/290 [==============================] - 0s 757us/step - loss: 0.1066\n",
      "Epoch 15/150\n",
      "290/290 [==============================] - 0s 737us/step - loss: 0.1055\n",
      "Epoch 16/150\n",
      "290/290 [==============================] - 0s 802us/step - loss: 0.1049\n",
      "Epoch 17/150\n",
      "290/290 [==============================] - 0s 638us/step - loss: 0.1062\n",
      "Epoch 18/150\n",
      "290/290 [==============================] - 0s 832us/step - loss: 0.1060\n",
      "Epoch 19/150\n",
      "290/290 [==============================] - 0s 713us/step - loss: 0.1031\n",
      "Epoch 20/150\n",
      "290/290 [==============================] - 0s 800us/step - loss: 0.1024\n",
      "Epoch 21/150\n",
      "290/290 [==============================] - 0s 754us/step - loss: 0.1023\n",
      "Epoch 22/150\n",
      "290/290 [==============================] - 0s 703us/step - loss: 0.1021\n",
      "Epoch 23/150\n",
      "290/290 [==============================] - 0s 716us/step - loss: 0.1015\n",
      "Epoch 24/150\n",
      "290/290 [==============================] - 0s 779us/step - loss: 0.1021\n",
      "Epoch 25/150\n",
      "290/290 [==============================] - 0s 763us/step - loss: 0.1003\n",
      "Epoch 26/150\n",
      "290/290 [==============================] - 0s 645us/step - loss: 0.0999\n",
      "Epoch 27/150\n",
      "290/290 [==============================] - 0s 916us/step - loss: 0.0997\n",
      "Epoch 28/150\n",
      "290/290 [==============================] - 0s 728us/step - loss: 0.0993\n",
      "Epoch 29/150\n",
      "290/290 [==============================] - 0s 732us/step - loss: 0.1004\n",
      "Epoch 30/150\n",
      "290/290 [==============================] - 0s 671us/step - loss: 0.0981\n",
      "Epoch 31/150\n",
      "290/290 [==============================] - 0s 695us/step - loss: 0.0987\n",
      "Epoch 32/150\n",
      "290/290 [==============================] - 0s 705us/step - loss: 0.0979\n",
      "Epoch 33/150\n",
      "290/290 [==============================] - 0s 884us/step - loss: 0.0977\n",
      "Epoch 34/150\n",
      "290/290 [==============================] - 0s 815us/step - loss: 0.0978\n",
      "Epoch 35/150\n",
      "290/290 [==============================] - 0s 789us/step - loss: 0.0972\n",
      "Epoch 36/150\n",
      "290/290 [==============================] - 0s 716us/step - loss: 0.0965\n",
      "Epoch 37/150\n",
      "290/290 [==============================] - 0s 846us/step - loss: 0.0976\n",
      "Epoch 38/150\n",
      "290/290 [==============================] - 0s 759us/step - loss: 0.0965\n",
      "Epoch 39/150\n",
      "290/290 [==============================] - 0s 733us/step - loss: 0.0961\n",
      "Epoch 40/150\n",
      "290/290 [==============================] - 0s 841us/step - loss: 0.0955\n",
      "Epoch 41/150\n",
      "290/290 [==============================] - 0s 686us/step - loss: 0.0956\n",
      "Epoch 42/150\n",
      "290/290 [==============================] - 0s 932us/step - loss: 0.0958\n",
      "Epoch 43/150\n",
      "290/290 [==============================] - 0s 804us/step - loss: 0.0956\n",
      "Epoch 44/150\n",
      "290/290 [==============================] - 0s 722us/step - loss: 0.0951\n",
      "Epoch 45/150\n",
      "290/290 [==============================] - 0s 852us/step - loss: 0.0950\n",
      "Epoch 46/150\n",
      "290/290 [==============================] - 0s 756us/step - loss: 0.0946\n",
      "Epoch 47/150\n",
      "290/290 [==============================] - 0s 1ms/step - loss: 0.0941\n",
      "Epoch 48/150\n",
      "290/290 [==============================] - 0s 740us/step - loss: 0.0944\n",
      "Epoch 49/150\n",
      "290/290 [==============================] - 0s 743us/step - loss: 0.0943 0s - loss: 0.09\n",
      "Epoch 50/150\n",
      "290/290 [==============================] - 0s 728us/step - loss: 0.0938\n",
      "Epoch 51/150\n",
      "290/290 [==============================] - ETA: 0s - loss: 0.093 - 0s 862us/step - loss: 0.0937\n",
      "Epoch 52/150\n",
      "290/290 [==============================] - 0s 2ms/step - loss: 0.0932\n",
      "Epoch 53/150\n",
      "290/290 [==============================] - 0s 948us/step - loss: 0.0932\n",
      "Epoch 54/150\n",
      "290/290 [==============================] - 0s 835us/step - loss: 0.0934\n",
      "Epoch 55/150\n",
      "290/290 [==============================] - 0s 918us/step - loss: 0.0938\n",
      "Epoch 56/150\n",
      "290/290 [==============================] - 0s 824us/step - loss: 0.0927\n",
      "Epoch 57/150\n",
      "290/290 [==============================] - 0s 775us/step - loss: 0.0932\n",
      "Epoch 58/150\n",
      "290/290 [==============================] - 0s 753us/step - loss: 0.0929\n",
      "Epoch 59/150\n",
      "290/290 [==============================] - 0s 753us/step - loss: 0.0927 0s - loss: 0.092\n",
      "Epoch 60/150\n",
      "290/290 [==============================] - 0s 750us/step - loss: 0.0922\n",
      "Epoch 61/150\n",
      "290/290 [==============================] - 0s 949us/step - loss: 0.0932\n",
      "Epoch 62/150\n",
      "290/290 [==============================] - 0s 897us/step - loss: 0.0920\n",
      "Epoch 63/150\n",
      "290/290 [==============================] - 0s 1ms/step - loss: 0.0934\n",
      "Epoch 64/150\n",
      "290/290 [==============================] - 1s 3ms/step - loss: 0.0917A: 0s - loss: 0\n",
      "Epoch 65/150\n",
      "290/290 [==============================] - 0s 1ms/step - loss: 0.0914\n",
      "Epoch 66/150\n",
      "290/290 [==============================] - 0s 2ms/step - loss: 0.0913\n",
      "Epoch 67/150\n",
      "290/290 [==============================] - 0s 1ms/step - loss: 0.0915\n",
      "Epoch 68/150\n",
      "290/290 [==============================] - 0s 1ms/step - loss: 0.0913\n",
      "Epoch 69/150\n",
      "290/290 [==============================] - 0s 960us/step - loss: 0.0911\n",
      "Epoch 70/150\n",
      "290/290 [==============================] - 0s 916us/step - loss: 0.0908\n",
      "Epoch 71/150\n",
      "290/290 [==============================] - 0s 998us/step - loss: 0.0909\n",
      "Epoch 72/150\n",
      "290/290 [==============================] - 0s 881us/step - loss: 0.0905\n",
      "Epoch 73/150\n",
      "290/290 [==============================] - 0s 1ms/step - loss: 0.0909\n",
      "Epoch 74/150\n",
      "290/290 [==============================] - 0s 888us/step - loss: 0.0907\n",
      "Epoch 75/150\n",
      "290/290 [==============================] - 0s 792us/step - loss: 0.0904\n",
      "Epoch 76/150\n",
      "290/290 [==============================] - 0s 791us/step - loss: 0.0911\n",
      "Epoch 77/150\n",
      "290/290 [==============================] - 0s 678us/step - loss: 0.0900\n",
      "Epoch 78/150\n",
      "290/290 [==============================] - 0s 741us/step - loss: 0.0907\n",
      "Epoch 79/150\n",
      "290/290 [==============================] - 0s 753us/step - loss: 0.0903\n",
      "Epoch 80/150\n",
      "290/290 [==============================] - 0s 859us/step - loss: 0.0900\n",
      "Epoch 81/150\n",
      "290/290 [==============================] - 0s 750us/step - loss: 0.0899\n",
      "Epoch 82/150\n",
      "290/290 [==============================] - 0s 684us/step - loss: 0.0895\n",
      "Epoch 83/150\n",
      "290/290 [==============================] - 0s 727us/step - loss: 0.0899\n",
      "Epoch 84/150\n",
      "290/290 [==============================] - 0s 747us/step - loss: 0.0892\n",
      "Epoch 85/150\n",
      "290/290 [==============================] - 0s 831us/step - loss: 0.0902\n",
      "Epoch 86/150\n",
      "290/290 [==============================] - 0s 794us/step - loss: 0.0900\n",
      "Epoch 87/150\n",
      "290/290 [==============================] - 0s 752us/step - loss: 0.0900\n",
      "Epoch 88/150\n",
      "290/290 [==============================] - 0s 757us/step - loss: 0.0893\n",
      "Epoch 89/150\n",
      "290/290 [==============================] - 0s 874us/step - loss: 0.0892\n",
      "Epoch 90/150\n",
      "290/290 [==============================] - 0s 793us/step - loss: 0.0893\n",
      "Epoch 91/150\n",
      "290/290 [==============================] - 0s 864us/step - loss: 0.0892\n",
      "Epoch 92/150\n",
      "290/290 [==============================] - 0s 848us/step - loss: 0.0893\n",
      "Epoch 93/150\n",
      "290/290 [==============================] - 0s 945us/step - loss: 0.0891\n",
      "Epoch 94/150\n",
      "290/290 [==============================] - 0s 801us/step - loss: 0.0893\n",
      "Epoch 95/150\n",
      "290/290 [==============================] - 0s 1ms/step - loss: 0.0891\n",
      "Epoch 96/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290/290 [==============================] - 0s 867us/step - loss: 0.0890\n",
      "Epoch 97/150\n",
      "290/290 [==============================] - 0s 757us/step - loss: 0.0891\n",
      "Epoch 98/150\n",
      "290/290 [==============================] - 0s 855us/step - loss: 0.0892\n",
      "Epoch 99/150\n",
      "290/290 [==============================] - 0s 1ms/step - loss: 0.0892\n",
      "Epoch 100/150\n",
      "290/290 [==============================] - 0s 585us/step - loss: 0.0891\n",
      "Epoch 101/150\n",
      "290/290 [==============================] - 0s 664us/step - loss: 0.0887\n",
      "Epoch 102/150\n",
      "290/290 [==============================] - 0s 564us/step - loss: 0.0895\n",
      "Epoch 103/150\n",
      "290/290 [==============================] - 0s 560us/step - loss: 0.0891\n",
      "Epoch 104/150\n",
      "290/290 [==============================] - 0s 522us/step - loss: 0.0885\n",
      "Epoch 105/150\n",
      "290/290 [==============================] - 0s 570us/step - loss: 0.0885\n",
      "Epoch 106/150\n",
      "290/290 [==============================] - 0s 599us/step - loss: 0.0889\n",
      "Epoch 107/150\n",
      "290/290 [==============================] - 0s 624us/step - loss: 0.0887\n",
      "Epoch 108/150\n",
      "290/290 [==============================] - 0s 697us/step - loss: 0.0884\n",
      "Epoch 109/150\n",
      "290/290 [==============================] - 0s 631us/step - loss: 0.0881\n",
      "Epoch 110/150\n",
      "290/290 [==============================] - 0s 603us/step - loss: 0.0879\n",
      "Epoch 111/150\n",
      "290/290 [==============================] - 0s 556us/step - loss: 0.0886\n",
      "Epoch 112/150\n",
      "290/290 [==============================] - 0s 355us/step - loss: 0.0885\n",
      "Epoch 113/150\n",
      "290/290 [==============================] - 0s 377us/step - loss: 0.0884\n",
      "Epoch 114/150\n",
      "290/290 [==============================] - 0s 377us/step - loss: 0.0889\n",
      "Epoch 115/150\n",
      "290/290 [==============================] - 0s 377us/step - loss: 0.0884\n",
      "Epoch 116/150\n",
      "290/290 [==============================] - 0s 594us/step - loss: 0.0886\n",
      "Epoch 117/150\n",
      "290/290 [==============================] - 0s 372us/step - loss: 0.0882\n",
      "Epoch 118/150\n",
      "290/290 [==============================] - 0s 356us/step - loss: 0.0881\n",
      "Epoch 119/150\n",
      "290/290 [==============================] - 0s 323us/step - loss: 0.0881\n",
      "Epoch 120/150\n",
      "290/290 [==============================] - 0s 377us/step - loss: 0.0883\n",
      "Epoch 121/150\n",
      "290/290 [==============================] - 0s 377us/step - loss: 0.0877\n",
      "Epoch 122/150\n",
      "290/290 [==============================] - 0s 431us/step - loss: 0.0882\n",
      "Epoch 123/150\n",
      "290/290 [==============================] - 0s 377us/step - loss: 0.0880\n",
      "Epoch 124/150\n",
      "290/290 [==============================] - 0s 323us/step - loss: 0.0875\n",
      "Epoch 125/150\n",
      "290/290 [==============================] - 0s 505us/step - loss: 0.0876\n",
      "Epoch 126/150\n",
      "290/290 [==============================] - 0s 503us/step - loss: 0.0889\n",
      "Epoch 127/150\n",
      "290/290 [==============================] - 0s 462us/step - loss: 0.0883\n",
      "Epoch 128/150\n",
      "290/290 [==============================] - 0s 527us/step - loss: 0.0873\n",
      "Epoch 129/150\n",
      "290/290 [==============================] - 0s 469us/step - loss: 0.0874\n",
      "Epoch 130/150\n",
      "290/290 [==============================] - 0s 377us/step - loss: 0.0874\n",
      "Epoch 131/150\n",
      "290/290 [==============================] - 0s 669us/step - loss: 0.0873\n",
      "Epoch 132/150\n",
      "290/290 [==============================] - 0s 420us/step - loss: 0.0879\n",
      "Epoch 133/150\n",
      "290/290 [==============================] - 0s 323us/step - loss: 0.0874\n",
      "Epoch 134/150\n",
      "290/290 [==============================] - 0s 377us/step - loss: 0.0871\n",
      "Epoch 135/150\n",
      "290/290 [==============================] - 0s 323us/step - loss: 0.0874\n",
      "Epoch 136/150\n",
      "290/290 [==============================] - 0s 377us/step - loss: 0.0877\n",
      "Epoch 137/150\n",
      "290/290 [==============================] - 0s 377us/step - loss: 0.0883\n",
      "Epoch 138/150\n",
      "290/290 [==============================] - 0s 485us/step - loss: 0.0876\n",
      "Epoch 139/150\n",
      "290/290 [==============================] - 0s 377us/step - loss: 0.0874\n",
      "Epoch 140/150\n",
      "290/290 [==============================] - 0s 371us/step - loss: 0.0879\n",
      "Epoch 141/150\n",
      "290/290 [==============================] - 0s 323us/step - loss: 0.0874\n",
      "Epoch 142/150\n",
      "290/290 [==============================] - 0s 807us/step - loss: 0.0870\n",
      "Epoch 143/150\n",
      "290/290 [==============================] - 0s 593us/step - loss: 0.0872\n",
      "Epoch 144/150\n",
      "290/290 [==============================] - 0s 647us/step - loss: 0.0881\n",
      "Epoch 145/150\n",
      "290/290 [==============================] - 0s 539us/step - loss: 0.0881\n",
      "Epoch 146/150\n",
      "290/290 [==============================] - 0s 539us/step - loss: 0.0875\n",
      "Epoch 147/150\n",
      "290/290 [==============================] - 0s 593us/step - loss: 0.0872\n",
      "Epoch 148/150\n",
      "290/290 [==============================] - 0s 647us/step - loss: 0.0868\n",
      "Epoch 149/150\n",
      "290/290 [==============================] - 0s 539us/step - loss: 0.0872\n",
      "Epoch 150/150\n",
      "290/290 [==============================] - 0s 593us/step - loss: 0.0870\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parsee = ct_sheet.sheet_names[2]\n",
    "data = ct_sheet.parse(parsee)\n",
    "data_features = data.loc[:, data.columns] \n",
    "data_features = data_features.drop(['ROI',11142,12142], axis=1)  \n",
    "\n",
    "parsee2 = ct_sheet.sheet_names[3]\n",
    "data2 = ct_sheet.parse(parsee2)\n",
    "data_labels = data2.loc[:, data2.columns] \n",
    "data_labels = data_labels.drop(['ROI',11142,12142], axis=1)  \n",
    "#Get rid of subject names to only have features now. #Need to remove ROIs. They don't convert to floats.\n",
    "#Get rid of ctx_rh_Medial_wall and ctx_lh_Medial_wall, not needed for analysis.\n",
    "#Have to standardize data. Scikit learn here. Need to create stratified K folds to avoid uneven distribution of risk groups.pcaCT1Y = PCA(n_components=150) #150 Features\n",
    "scaler_filename = \"IBIS_scaledSA1y.save\"\n",
    "scaler = joblib.load(scaler_filename)\n",
    "scaled_data_1y = scaler.transform(data_features)\n",
    "\n",
    "scaler_filename2 = \"IBIS_scaledSA2y.save\"\n",
    "scaler2 = joblib.load(scaler_filename2)\n",
    "scaled_data_2y = scaler.transform(data_labels)\n",
    "print(scaled_data_1y.shape)\n",
    "print(scaled_data_2y.shape)\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(scaled_data, scaled_labels, test_size=0.10, random_state=20)\n",
    "\n",
    "#Size of encoded representation\n",
    "#{'batch_size': 10, 'dropout': 0.15, 'encoded_layer_size': 25, 'epochs': 150, 'layer1_size': 100, 'layer2_size': 40}\n",
    "input_size = 148\n",
    "hidden_size = 100\n",
    "hidden_size_2 = 40\n",
    "encoding_dim = 25\n",
    "dropout = 0.15\n",
    "\n",
    "# Input Placeholder\n",
    "input_data = Input(shape=(input_size,))\n",
    "print(input_data)\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "hidden_e_1 = Dense(hidden_size, activation='tanh')(input_data) \n",
    "hidden_e_2 = Dense(hidden_size_2, activation='tanh')(hidden_e_1)\n",
    "dropout_layer = Dropout(dropout)(hidden_e_2)\n",
    "encoded = Dense(encoding_dim, activation='tanh')(dropout_layer)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "hidden_d_1 = Dense(hidden_size, activation='tanh')(encoded)\n",
    "dropout_layer_d = Dropout(dropout)(hidden_d_1)\n",
    "hidden_d_2 = Dense(hidden_size, activation='tanh')(dropout_layer_d)\n",
    "decoded = Dense(input_size, activation='tanh')(hidden_d_2) \n",
    "# this model maps an input to its prediction\n",
    "autoencoder = Model(input_data, decoded)\n",
    "# configure our model to use mean_absolute_error loss function, and the Adam optimizer:\n",
    "autoencoder.compile(optimizer='Adam', loss='mean_absolute_error')\n",
    "\n",
    "ac = autoencoder.fit(scaled_data_2y, scaled_data_1y,\n",
    "epochs=150,\n",
    "batch_size=10,\n",
    "shuffle=True)\n",
    "\n",
    "#print(ac.history.keys())\n",
    "# \"Loss\"\n",
    "plt.plot(ac.history['loss'])\n",
    "#plt.plot(ac.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "#plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.axis([0, 150, 0.0, 0.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save(\"Autoencoder Input 2 year Output 1 year SA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2Y SA\n",
      "(20, 148)\n"
     ]
    }
   ],
   "source": [
    "sa_sheet_2y = pd.ExcelFile(\"Data to be Interpolated.xlsx\") \n",
    "parsee = sa_sheet_2y.sheet_names[7]\n",
    "print(parsee)\n",
    "data = sa_sheet_2y.parse(parsee)\n",
    "data_features_sa2y = data.loc[:, data.columns] \n",
    "data_features_sa2y = data_features_sa2y.drop(['ROI',11142,12142], axis=1)\n",
    "scaled_data_sa2y = scaler2.transform(data_features_sa2y)\n",
    "print(scaled_data_sa2y.shape)\n",
    "\n",
    "predicted_1yr_sa = autoencoder.predict(scaled_data_sa2y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1810903  0.39202335 0.33550692 ... 0.09240081 0.03404321 0.19359294]\n",
      " [0.27058756 0.2375618  0.34840542 ... 0.2521816  0.21622612 0.5516302 ]\n",
      " [0.06338966 0.19653668 0.34986326 ... 0.41719174 0.20715883 0.18120831]\n",
      " ...\n",
      " [0.21405077 0.33600974 0.31263492 ... 0.15139459 0.1626573  0.26953456]\n",
      " [0.17343849 0.27491552 0.37982374 ... 0.17247915 0.23904334 0.53592944]\n",
      " [0.17247982 0.35886678 0.29976493 ... 0.17308249 0.32910112 0.38326564]]\n"
     ]
    }
   ],
   "source": [
    "print(predicted_1yr_sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(predicted_1yr_sa)\n",
    "df.to_excel(\"Interpolated SA 1y.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.181090</td>\n",
       "      <td>0.392023</td>\n",
       "      <td>0.335507</td>\n",
       "      <td>0.082010</td>\n",
       "      <td>0.180316</td>\n",
       "      <td>0.109730</td>\n",
       "      <td>0.139186</td>\n",
       "      <td>0.269446</td>\n",
       "      <td>0.069798</td>\n",
       "      <td>0.132574</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288397</td>\n",
       "      <td>0.060133</td>\n",
       "      <td>0.021577</td>\n",
       "      <td>0.077016</td>\n",
       "      <td>0.158788</td>\n",
       "      <td>0.109464</td>\n",
       "      <td>0.071428</td>\n",
       "      <td>0.092401</td>\n",
       "      <td>0.034043</td>\n",
       "      <td>0.193593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.270588</td>\n",
       "      <td>0.237562</td>\n",
       "      <td>0.348405</td>\n",
       "      <td>0.382680</td>\n",
       "      <td>0.306109</td>\n",
       "      <td>0.246741</td>\n",
       "      <td>0.314882</td>\n",
       "      <td>0.212238</td>\n",
       "      <td>0.276413</td>\n",
       "      <td>0.208346</td>\n",
       "      <td>...</td>\n",
       "      <td>0.387932</td>\n",
       "      <td>0.361144</td>\n",
       "      <td>0.253834</td>\n",
       "      <td>0.254250</td>\n",
       "      <td>0.157880</td>\n",
       "      <td>0.159473</td>\n",
       "      <td>0.241695</td>\n",
       "      <td>0.252182</td>\n",
       "      <td>0.216226</td>\n",
       "      <td>0.551630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.063390</td>\n",
       "      <td>0.196537</td>\n",
       "      <td>0.349863</td>\n",
       "      <td>0.183645</td>\n",
       "      <td>0.369511</td>\n",
       "      <td>0.286652</td>\n",
       "      <td>0.194375</td>\n",
       "      <td>0.403243</td>\n",
       "      <td>0.142836</td>\n",
       "      <td>0.132978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.614595</td>\n",
       "      <td>0.232842</td>\n",
       "      <td>0.297941</td>\n",
       "      <td>0.182558</td>\n",
       "      <td>0.182359</td>\n",
       "      <td>0.210807</td>\n",
       "      <td>0.251910</td>\n",
       "      <td>0.417192</td>\n",
       "      <td>0.207159</td>\n",
       "      <td>0.181208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.236520</td>\n",
       "      <td>0.350521</td>\n",
       "      <td>0.368630</td>\n",
       "      <td>0.349298</td>\n",
       "      <td>0.305270</td>\n",
       "      <td>0.325958</td>\n",
       "      <td>0.328949</td>\n",
       "      <td>0.284367</td>\n",
       "      <td>0.241415</td>\n",
       "      <td>0.220600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.549584</td>\n",
       "      <td>0.337069</td>\n",
       "      <td>0.340731</td>\n",
       "      <td>0.248773</td>\n",
       "      <td>0.233169</td>\n",
       "      <td>0.223312</td>\n",
       "      <td>0.293626</td>\n",
       "      <td>0.293540</td>\n",
       "      <td>0.321982</td>\n",
       "      <td>0.269697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.098037</td>\n",
       "      <td>0.143809</td>\n",
       "      <td>0.328337</td>\n",
       "      <td>0.236191</td>\n",
       "      <td>0.300728</td>\n",
       "      <td>0.159263</td>\n",
       "      <td>0.317377</td>\n",
       "      <td>0.171072</td>\n",
       "      <td>0.227620</td>\n",
       "      <td>0.242329</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281991</td>\n",
       "      <td>0.311132</td>\n",
       "      <td>0.210620</td>\n",
       "      <td>0.143039</td>\n",
       "      <td>0.132149</td>\n",
       "      <td>0.122241</td>\n",
       "      <td>0.156450</td>\n",
       "      <td>0.276244</td>\n",
       "      <td>0.102099</td>\n",
       "      <td>0.362651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.187891</td>\n",
       "      <td>0.284216</td>\n",
       "      <td>0.432154</td>\n",
       "      <td>0.346139</td>\n",
       "      <td>0.265192</td>\n",
       "      <td>0.313286</td>\n",
       "      <td>0.283571</td>\n",
       "      <td>0.280694</td>\n",
       "      <td>0.360329</td>\n",
       "      <td>0.314395</td>\n",
       "      <td>...</td>\n",
       "      <td>0.378622</td>\n",
       "      <td>0.366899</td>\n",
       "      <td>0.318536</td>\n",
       "      <td>0.240848</td>\n",
       "      <td>0.187533</td>\n",
       "      <td>0.249052</td>\n",
       "      <td>0.300728</td>\n",
       "      <td>0.272838</td>\n",
       "      <td>0.310437</td>\n",
       "      <td>0.508997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.253132</td>\n",
       "      <td>0.312364</td>\n",
       "      <td>0.308020</td>\n",
       "      <td>0.237077</td>\n",
       "      <td>0.384794</td>\n",
       "      <td>0.286935</td>\n",
       "      <td>0.330396</td>\n",
       "      <td>0.403875</td>\n",
       "      <td>0.265358</td>\n",
       "      <td>0.173825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.510529</td>\n",
       "      <td>0.336888</td>\n",
       "      <td>0.226504</td>\n",
       "      <td>0.183483</td>\n",
       "      <td>0.199213</td>\n",
       "      <td>0.230928</td>\n",
       "      <td>0.314380</td>\n",
       "      <td>0.238644</td>\n",
       "      <td>0.241366</td>\n",
       "      <td>0.217476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.188943</td>\n",
       "      <td>0.397495</td>\n",
       "      <td>0.379319</td>\n",
       "      <td>0.307837</td>\n",
       "      <td>0.243962</td>\n",
       "      <td>0.164570</td>\n",
       "      <td>0.210961</td>\n",
       "      <td>0.257771</td>\n",
       "      <td>0.163571</td>\n",
       "      <td>0.252756</td>\n",
       "      <td>...</td>\n",
       "      <td>0.362905</td>\n",
       "      <td>0.231417</td>\n",
       "      <td>0.302511</td>\n",
       "      <td>0.204325</td>\n",
       "      <td>0.156752</td>\n",
       "      <td>0.185205</td>\n",
       "      <td>0.306530</td>\n",
       "      <td>0.256978</td>\n",
       "      <td>0.232506</td>\n",
       "      <td>0.406381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.151888</td>\n",
       "      <td>0.167940</td>\n",
       "      <td>0.303540</td>\n",
       "      <td>0.279281</td>\n",
       "      <td>0.452564</td>\n",
       "      <td>0.340428</td>\n",
       "      <td>0.364224</td>\n",
       "      <td>0.464117</td>\n",
       "      <td>0.216340</td>\n",
       "      <td>0.247560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.626041</td>\n",
       "      <td>0.361752</td>\n",
       "      <td>0.345750</td>\n",
       "      <td>0.304476</td>\n",
       "      <td>0.228573</td>\n",
       "      <td>0.148072</td>\n",
       "      <td>0.200470</td>\n",
       "      <td>0.281836</td>\n",
       "      <td>0.285306</td>\n",
       "      <td>0.064791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.201543</td>\n",
       "      <td>0.275800</td>\n",
       "      <td>0.329969</td>\n",
       "      <td>0.366613</td>\n",
       "      <td>0.411284</td>\n",
       "      <td>0.252403</td>\n",
       "      <td>0.306975</td>\n",
       "      <td>0.404991</td>\n",
       "      <td>0.198927</td>\n",
       "      <td>0.235115</td>\n",
       "      <td>...</td>\n",
       "      <td>0.469910</td>\n",
       "      <td>0.333444</td>\n",
       "      <td>0.403349</td>\n",
       "      <td>0.257372</td>\n",
       "      <td>0.163551</td>\n",
       "      <td>0.123668</td>\n",
       "      <td>0.250836</td>\n",
       "      <td>0.283224</td>\n",
       "      <td>0.266941</td>\n",
       "      <td>0.228069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.149652</td>\n",
       "      <td>0.279627</td>\n",
       "      <td>0.321899</td>\n",
       "      <td>0.240318</td>\n",
       "      <td>0.301922</td>\n",
       "      <td>0.247648</td>\n",
       "      <td>0.276895</td>\n",
       "      <td>0.349303</td>\n",
       "      <td>0.172080</td>\n",
       "      <td>0.237963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.528278</td>\n",
       "      <td>0.293350</td>\n",
       "      <td>0.256288</td>\n",
       "      <td>0.119808</td>\n",
       "      <td>0.189349</td>\n",
       "      <td>0.166900</td>\n",
       "      <td>0.217235</td>\n",
       "      <td>0.261865</td>\n",
       "      <td>0.136767</td>\n",
       "      <td>0.206709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.212401</td>\n",
       "      <td>0.195853</td>\n",
       "      <td>0.355814</td>\n",
       "      <td>0.320660</td>\n",
       "      <td>0.313023</td>\n",
       "      <td>0.250343</td>\n",
       "      <td>0.324955</td>\n",
       "      <td>0.200587</td>\n",
       "      <td>0.224647</td>\n",
       "      <td>0.191956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.684127</td>\n",
       "      <td>0.273462</td>\n",
       "      <td>0.073456</td>\n",
       "      <td>0.189009</td>\n",
       "      <td>0.235562</td>\n",
       "      <td>0.177504</td>\n",
       "      <td>0.184635</td>\n",
       "      <td>0.388793</td>\n",
       "      <td>0.123042</td>\n",
       "      <td>0.457252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.185067</td>\n",
       "      <td>0.169920</td>\n",
       "      <td>0.285919</td>\n",
       "      <td>0.197333</td>\n",
       "      <td>0.290437</td>\n",
       "      <td>0.203663</td>\n",
       "      <td>0.260474</td>\n",
       "      <td>0.325279</td>\n",
       "      <td>0.176138</td>\n",
       "      <td>0.226738</td>\n",
       "      <td>...</td>\n",
       "      <td>0.517392</td>\n",
       "      <td>0.206291</td>\n",
       "      <td>0.079368</td>\n",
       "      <td>0.173231</td>\n",
       "      <td>0.178528</td>\n",
       "      <td>0.086074</td>\n",
       "      <td>0.115494</td>\n",
       "      <td>0.178533</td>\n",
       "      <td>0.148336</td>\n",
       "      <td>0.202529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.211938</td>\n",
       "      <td>0.298471</td>\n",
       "      <td>0.353406</td>\n",
       "      <td>0.214463</td>\n",
       "      <td>0.264183</td>\n",
       "      <td>0.215549</td>\n",
       "      <td>0.298681</td>\n",
       "      <td>0.252756</td>\n",
       "      <td>0.305827</td>\n",
       "      <td>0.281470</td>\n",
       "      <td>...</td>\n",
       "      <td>0.475835</td>\n",
       "      <td>0.303031</td>\n",
       "      <td>0.195548</td>\n",
       "      <td>0.023033</td>\n",
       "      <td>0.150838</td>\n",
       "      <td>0.186579</td>\n",
       "      <td>0.301743</td>\n",
       "      <td>0.186388</td>\n",
       "      <td>0.236179</td>\n",
       "      <td>0.452879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.218309</td>\n",
       "      <td>0.400204</td>\n",
       "      <td>0.372537</td>\n",
       "      <td>0.294198</td>\n",
       "      <td>0.227661</td>\n",
       "      <td>0.207725</td>\n",
       "      <td>0.351837</td>\n",
       "      <td>0.305983</td>\n",
       "      <td>0.299985</td>\n",
       "      <td>0.226289</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430307</td>\n",
       "      <td>0.342003</td>\n",
       "      <td>0.103098</td>\n",
       "      <td>0.158627</td>\n",
       "      <td>0.157795</td>\n",
       "      <td>0.177783</td>\n",
       "      <td>0.102930</td>\n",
       "      <td>0.033001</td>\n",
       "      <td>0.317755</td>\n",
       "      <td>0.295952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.182772</td>\n",
       "      <td>0.371373</td>\n",
       "      <td>0.283429</td>\n",
       "      <td>0.262591</td>\n",
       "      <td>0.319971</td>\n",
       "      <td>0.178785</td>\n",
       "      <td>0.226146</td>\n",
       "      <td>0.394804</td>\n",
       "      <td>0.152644</td>\n",
       "      <td>0.177128</td>\n",
       "      <td>...</td>\n",
       "      <td>0.410642</td>\n",
       "      <td>0.339397</td>\n",
       "      <td>0.284478</td>\n",
       "      <td>0.315321</td>\n",
       "      <td>0.178424</td>\n",
       "      <td>0.163108</td>\n",
       "      <td>0.169450</td>\n",
       "      <td>0.090452</td>\n",
       "      <td>0.279956</td>\n",
       "      <td>0.176573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.263590</td>\n",
       "      <td>0.362857</td>\n",
       "      <td>0.366648</td>\n",
       "      <td>0.366503</td>\n",
       "      <td>0.444475</td>\n",
       "      <td>0.282528</td>\n",
       "      <td>0.319373</td>\n",
       "      <td>0.445090</td>\n",
       "      <td>0.306263</td>\n",
       "      <td>0.266781</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423848</td>\n",
       "      <td>0.368375</td>\n",
       "      <td>0.221991</td>\n",
       "      <td>0.266305</td>\n",
       "      <td>0.190395</td>\n",
       "      <td>0.170650</td>\n",
       "      <td>0.233030</td>\n",
       "      <td>0.120574</td>\n",
       "      <td>0.315787</td>\n",
       "      <td>0.438400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.214051</td>\n",
       "      <td>0.336010</td>\n",
       "      <td>0.312635</td>\n",
       "      <td>0.176775</td>\n",
       "      <td>0.225478</td>\n",
       "      <td>0.077186</td>\n",
       "      <td>0.116375</td>\n",
       "      <td>0.219387</td>\n",
       "      <td>0.129854</td>\n",
       "      <td>0.176415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.254123</td>\n",
       "      <td>0.141662</td>\n",
       "      <td>0.240632</td>\n",
       "      <td>0.148930</td>\n",
       "      <td>0.110003</td>\n",
       "      <td>0.139221</td>\n",
       "      <td>0.253018</td>\n",
       "      <td>0.151395</td>\n",
       "      <td>0.162657</td>\n",
       "      <td>0.269535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.173438</td>\n",
       "      <td>0.274916</td>\n",
       "      <td>0.379824</td>\n",
       "      <td>0.290506</td>\n",
       "      <td>0.278949</td>\n",
       "      <td>0.253989</td>\n",
       "      <td>0.278606</td>\n",
       "      <td>0.251514</td>\n",
       "      <td>0.315935</td>\n",
       "      <td>0.202020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430195</td>\n",
       "      <td>0.397511</td>\n",
       "      <td>0.223799</td>\n",
       "      <td>0.258644</td>\n",
       "      <td>0.181407</td>\n",
       "      <td>0.183620</td>\n",
       "      <td>0.168828</td>\n",
       "      <td>0.172479</td>\n",
       "      <td>0.239043</td>\n",
       "      <td>0.535929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.172480</td>\n",
       "      <td>0.358867</td>\n",
       "      <td>0.299765</td>\n",
       "      <td>0.359503</td>\n",
       "      <td>0.235570</td>\n",
       "      <td>0.170146</td>\n",
       "      <td>0.219707</td>\n",
       "      <td>0.299654</td>\n",
       "      <td>0.204542</td>\n",
       "      <td>0.263322</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270742</td>\n",
       "      <td>0.280740</td>\n",
       "      <td>0.277662</td>\n",
       "      <td>0.268112</td>\n",
       "      <td>0.158811</td>\n",
       "      <td>0.151156</td>\n",
       "      <td>0.224929</td>\n",
       "      <td>0.173082</td>\n",
       "      <td>0.329101</td>\n",
       "      <td>0.383266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows  148 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \\\n",
       "0   0.181090  0.392023  0.335507  0.082010  0.180316  0.109730  0.139186   \n",
       "1   0.270588  0.237562  0.348405  0.382680  0.306109  0.246741  0.314882   \n",
       "2   0.063390  0.196537  0.349863  0.183645  0.369511  0.286652  0.194375   \n",
       "3   0.236520  0.350521  0.368630  0.349298  0.305270  0.325958  0.328949   \n",
       "4   0.098037  0.143809  0.328337  0.236191  0.300728  0.159263  0.317377   \n",
       "5   0.187891  0.284216  0.432154  0.346139  0.265192  0.313286  0.283571   \n",
       "6   0.253132  0.312364  0.308020  0.237077  0.384794  0.286935  0.330396   \n",
       "7   0.188943  0.397495  0.379319  0.307837  0.243962  0.164570  0.210961   \n",
       "8   0.151888  0.167940  0.303540  0.279281  0.452564  0.340428  0.364224   \n",
       "9   0.201543  0.275800  0.329969  0.366613  0.411284  0.252403  0.306975   \n",
       "10  0.149652  0.279627  0.321899  0.240318  0.301922  0.247648  0.276895   \n",
       "11  0.212401  0.195853  0.355814  0.320660  0.313023  0.250343  0.324955   \n",
       "12  0.185067  0.169920  0.285919  0.197333  0.290437  0.203663  0.260474   \n",
       "13  0.211938  0.298471  0.353406  0.214463  0.264183  0.215549  0.298681   \n",
       "14  0.218309  0.400204  0.372537  0.294198  0.227661  0.207725  0.351837   \n",
       "15  0.182772  0.371373  0.283429  0.262591  0.319971  0.178785  0.226146   \n",
       "16  0.263590  0.362857  0.366648  0.366503  0.444475  0.282528  0.319373   \n",
       "17  0.214051  0.336010  0.312635  0.176775  0.225478  0.077186  0.116375   \n",
       "18  0.173438  0.274916  0.379824  0.290506  0.278949  0.253989  0.278606   \n",
       "19  0.172480  0.358867  0.299765  0.359503  0.235570  0.170146  0.219707   \n",
       "\n",
       "         7         8         9    ...       138       139       140       141  \\\n",
       "0   0.269446  0.069798  0.132574  ...  0.288397  0.060133  0.021577  0.077016   \n",
       "1   0.212238  0.276413  0.208346  ...  0.387932  0.361144  0.253834  0.254250   \n",
       "2   0.403243  0.142836  0.132978  ...  0.614595  0.232842  0.297941  0.182558   \n",
       "3   0.284367  0.241415  0.220600  ...  0.549584  0.337069  0.340731  0.248773   \n",
       "4   0.171072  0.227620  0.242329  ...  0.281991  0.311132  0.210620  0.143039   \n",
       "5   0.280694  0.360329  0.314395  ...  0.378622  0.366899  0.318536  0.240848   \n",
       "6   0.403875  0.265358  0.173825  ...  0.510529  0.336888  0.226504  0.183483   \n",
       "7   0.257771  0.163571  0.252756  ...  0.362905  0.231417  0.302511  0.204325   \n",
       "8   0.464117  0.216340  0.247560  ...  0.626041  0.361752  0.345750  0.304476   \n",
       "9   0.404991  0.198927  0.235115  ...  0.469910  0.333444  0.403349  0.257372   \n",
       "10  0.349303  0.172080  0.237963  ...  0.528278  0.293350  0.256288  0.119808   \n",
       "11  0.200587  0.224647  0.191956  ...  0.684127  0.273462  0.073456  0.189009   \n",
       "12  0.325279  0.176138  0.226738  ...  0.517392  0.206291  0.079368  0.173231   \n",
       "13  0.252756  0.305827  0.281470  ...  0.475835  0.303031  0.195548  0.023033   \n",
       "14  0.305983  0.299985  0.226289  ...  0.430307  0.342003  0.103098  0.158627   \n",
       "15  0.394804  0.152644  0.177128  ...  0.410642  0.339397  0.284478  0.315321   \n",
       "16  0.445090  0.306263  0.266781  ...  0.423848  0.368375  0.221991  0.266305   \n",
       "17  0.219387  0.129854  0.176415  ...  0.254123  0.141662  0.240632  0.148930   \n",
       "18  0.251514  0.315935  0.202020  ...  0.430195  0.397511  0.223799  0.258644   \n",
       "19  0.299654  0.204542  0.263322  ...  0.270742  0.280740  0.277662  0.268112   \n",
       "\n",
       "         142       143       144       145       146       147  \n",
       "0   0.158788  0.109464  0.071428  0.092401  0.034043  0.193593  \n",
       "1   0.157880  0.159473  0.241695  0.252182  0.216226  0.551630  \n",
       "2   0.182359  0.210807  0.251910  0.417192  0.207159  0.181208  \n",
       "3   0.233169  0.223312  0.293626  0.293540  0.321982  0.269697  \n",
       "4   0.132149  0.122241  0.156450  0.276244  0.102099  0.362651  \n",
       "5   0.187533  0.249052  0.300728  0.272838  0.310437  0.508997  \n",
       "6   0.199213  0.230928  0.314380  0.238644  0.241366  0.217476  \n",
       "7   0.156752  0.185205  0.306530  0.256978  0.232506  0.406381  \n",
       "8   0.228573  0.148072  0.200470  0.281836  0.285306  0.064791  \n",
       "9   0.163551  0.123668  0.250836  0.283224  0.266941  0.228069  \n",
       "10  0.189349  0.166900  0.217235  0.261865  0.136767  0.206709  \n",
       "11  0.235562  0.177504  0.184635  0.388793  0.123042  0.457252  \n",
       "12  0.178528  0.086074  0.115494  0.178533  0.148336  0.202529  \n",
       "13  0.150838  0.186579  0.301743  0.186388  0.236179  0.452879  \n",
       "14  0.157795  0.177783  0.102930  0.033001  0.317755  0.295952  \n",
       "15  0.178424  0.163108  0.169450  0.090452  0.279956  0.176573  \n",
       "16  0.190395  0.170650  0.233030  0.120574  0.315787  0.438400  \n",
       "17  0.110003  0.139221  0.253018  0.151395  0.162657  0.269535  \n",
       "18  0.181407  0.183620  0.168828  0.172479  0.239043  0.535929  \n",
       "19  0.158811  0.151156  0.224929  0.173082  0.329101  0.383266  \n",
       "\n",
       "[20 rows x 148 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SA2y\n",
      "(188, 148)\n"
     ]
    }
   ],
   "source": [
    "#Now calculate MAE on the original Gilmore Dataset\n",
    "gilmore_sheet = pd.ExcelFile(\"Gilmore CT1y2y SA1y2y.xlsx\") \n",
    "parsee = gilmore_sheet.sheet_names[3] #sa2y\n",
    "print(parsee)\n",
    "data = gilmore_sheet.parse(parsee)\n",
    "data_features_sa2y = data.loc[:, data.columns] \n",
    "data_features_sa2y = data_features_sa2y.drop(['ROI'], axis=1)\n",
    "scaled_data_sa2y_gilmore = scaler.transform(data_features_sa2y)\n",
    "print(scaled_data_sa2y_gilmore.shape)\n",
    "\n",
    "predicted_1yr_sa_gilmore = autoencoder.predict(scaled_data_sa2y_gilmore)\n",
    "df = pd.DataFrame(predicted_1yr_sa_gilmore)\n",
    "df.to_excel(\"Interpolated SA 1y Gilmore Scaled.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inverse Transform the predicted data to calculate MAE\n",
    "unscaled_predicted_sa1y = scaler.inverse_transform(predicted_1yr_sa_gilmore)\n",
    "type(unscaled_predicted_sa1y)\n",
    "df = pd.DataFrame(unscaled_predicted_sa1y)\n",
    "df.to_excel(\"Interpolated SA 1y Gilmore.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
